import tensorflow as tf
from tensorflow.keras import models,layers
import numpy as np
import matplotlib.pyplot as plt
import os

# Set the path to your Indian Sign Language dataset
DATASET_PATH = "./Indian"  # Change this to your actual dataset path

if not os.path.exists(DATASET_PATH):
    print(f"ERROR: Dataset not found at {DATASET_PATH}")
    print("Please download the Indian Sign Language dataset and place it in the 'data' folder.")
    print("Expected structure: ./Indian/<class_name>/<images>")
    exit(1)

data = tf.keras.utils.image_dataset_from_directory(

    DATASET_PATH,
    shuffle =True,
    image_size = (128, 128),
    batch_size =4
)

class_name = data.class_names
print(class_name)

print(len(data))

for image_batch,label_batch in data.take(1):
  print(image_batch.shape)
  print(label_batch[0].numpy())

for image_size,label in data.take(1):
  # print(image_size)
  print(image_size.ndim)
  # print(label)

for image_batch,label_batch in data.take(1):    
    plt.imshow(image_batch[0].numpy().astype('uint8'))
    plt.title(class_name[label_batch[0]])

train_ds = data.take(1036) 
print(len(train_ds))

test_ds = data.skip(1036).take(150)
print(len(test_ds))

valid_ds = data.skip(1186).take(150)
print(len(valid_ds))

print(len(train_ds))
print(len(test_ds))
print(len(valid_ds))      

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
valid_ds = valid_ds.prefetch(buffer_size=AUTOTUNE)

IMAGE_SIZE = 128  # Match the loaded image size
resize_and_rescale = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1.0/255)  # Normalize pixel values to [0, 1]
])

data_augment = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal_and_vertical"),
    tf.keras.layers.RandomRotation(0.2)
])



tf.config.optimizer.set_experimental_options({'layout_optimizer': False})


BATCH_SIZE = 32
IMAGE_SIZE = 128  # Must match image_dataset_from_directory size
CHANNELS=3
EPOCHS=10
input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
n_classes = len(class_name)

# NEURAL NETWORK STRUCTURE

from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Input(shape=input_shape),  #  Add this single Input layer at the start
    resize_and_rescale,               # your preprocessing layer
    data_augment,                     # your augmentation layer

    # Block 1
    layers.Conv2D(32, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(32, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    # Block 2
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.30),

    # Block 3
    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.40),

    # Classification Head
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.50),
    layers.Dense(n_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

history = model.fit(
    train_ds,
    validation_data=valid_ds,
    epochs=9
)

# ==================== EVALUATE AND PRINT STATISTICS ====================
print("\n" + "="*60)
print("MODEL TRAINING COMPLETE - ACCURACY AND STATISTICS")
print("="*60)

# Evaluate on validation set
val_loss, val_accuracy = model.evaluate(valid_ds, verbose=0)
print(f"\nValidation Accuracy: {val_accuracy*100:.2f}%")
print(f"Validation Loss: {val_loss:.4f}")

# Get training history
if history:
    train_accuracy = history.history['accuracy'][-1]
    train_loss = history.history['loss'][-1]
    val_acc_history = history.history['val_accuracy'][-1]
    val_loss_history = history.history['val_loss'][-1]
    
    print(f"\nFinal Training Accuracy: {train_accuracy*100:.2f}%")
    print(f"Final Training Loss: {train_loss:.4f}")
    print(f"Final Validation Accuracy: {val_acc_history*100:.2f}%")
    print(f"Final Validation Loss: {val_loss_history:.4f}")
    
    # Calculate improvement
    improvement = (val_acc_history - history.history['accuracy'][0]) * 100
    print(f"\nAccuracy Improvement: {improvement:.2f}%")
    
    # Best accuracy
    best_val_accuracy = max(history.history['val_accuracy'])
    best_epoch = history.history['val_accuracy'].index(best_val_accuracy) + 1
    print(f"Best Validation Accuracy: {best_val_accuracy*100:.2f}% (at epoch {best_epoch})")

print(f"\nTotal Classes: {n_classes}")
print(f"Classes: {class_name}")
print("="*60)